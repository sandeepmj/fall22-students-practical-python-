{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","colab":{"name":"week-8C-download-and-read_BLANK.ipynb","provenance":[{"file_id":"1O76yK1JGDPc1kRTHTOIDjev2XbLGExhv","timestamp":1646785092936},{"file_id":"1U8zDTAuA8NQ2JaDzSqzSn03P2iT8KFLG","timestamp":1639579061177},{"file_id":"1tC30RwvbvYZQU9bLZ75I6kBbMski9LFN","timestamp":1628626954402}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fv1F1HyA9zmX"},"source":["# Capture and organize data in downloaded files "]},{"cell_type":"markdown","metadata":{"id":"EsXo4Upb9zmb"},"source":["Let's download these ```.txt``` and ```.pdf``` files in our most recent scrape homework. \n","\n","We will read the text files using Python and export data to csv. (are you seeing a pattern yet?)"]},{"cell_type":"markdown","source":["### pip install necessary packages"],"metadata":{"id":"qXjbiXpKD_zc"}},{"cell_type":"code","source":["pip install wget"],"metadata":{"id":"cEVjNvm8DdJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install icecream "],"metadata":{"id":"mRtnH4saEJxB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lyvc6CPd-FUC"},"source":["# Import libraries\n"]},{"cell_type":"code","metadata":{"id":"m2ru6dyM9zmb"},"source":["## in order to export our file to our computer drive, you need this only in Colab:\n","# import libraries\n","from bs4 import BeautifulSoup  ## scrape info from web pages\n","import requests ## get web pages from server\n","import time # time is required. we will use its sleep function\n","from random import randrange # generate random numbers\n","from icecream import ic\n","from google.colab import files ## code for downloading in google colab\n","import pandas as pd ## to export csv file\n","import itertools ## import itertools\n","import wget\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# url to scrape\n","url = \"https://sandeepmj.github.io/scrape-example-page/pages.html\""],"metadata":{"id":"nUXBF742C5yI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## get url and print but hard to read. will do prettify next\n","response = requests.get(url)\n","response.status_code\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","soup"],"metadata":{"id":"eMotK_2jC9aP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## find all links to files in our soup\n","all_files = soup.find_all(\"ul\", class_=\"downloadable\")\n","all_files"],"metadata":{"id":"Vki1iZgTC__o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## iterate through our list\n","## to find just the a tags \n","all_a_tags = [file.find_all(\"a\") for file in all_files]\n","all_a_tags"],"metadata":{"id":"KAt7clyDDCHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## use itertools to flatten the list\n","\n","flat_list = list(itertools.chain(*all_a_tags))\n","flat_list"],"metadata":{"id":"2EpbRVzNDNYm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## now we can target the a tags\n","\n","href_list = [a_tag.get(\"href\") for a_tag in flat_list]\n","href_list"],"metadata":{"id":"ruhv4olrDQrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## base url\n","base_url = \"https://sandeepmj.github.io/scrape-example-page/\""],"metadata":{"id":"zncncauiDTBg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## iterate and join the base url to the relative url\n","full_link_list = [base_url + href for href in href_list]\n","full_link_list"],"metadata":{"id":"h-4oz2qfDV4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## full scrape of the documents\n","\n","links_number = len(full_link_list)\n","link_count = 1\n","for link in full_link_list:\n","  print(f\"Downloading link {link_count} of {links_number}\")\n","  link_count += 1\n","  files.download(wget.download(link))## wget function\n","  snooze = randrange(3, 6)\n","  print(f\"Snoozing for {snooze} seconds from next link\")\n","  time.sleep(snooze)"],"metadata":{"id":"cizZM_bODYpR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Giving structure to unstructured data 1"],"metadata":{"id":"lJMGDK6Uh5-h"}},{"cell_type":"code","source":["## pull all the txt files into our notebook"],"metadata":{"id":"yFKQYyAiGbYH"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqN9xmTuD-Ao"},"source":["## let's see what the first line of each file contains\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmZYM2D4D99U"},"source":["## let's see what each entire file contains\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OdcgUwoknrrd"},"source":["## We can interpret this ```<class '_io.TextIOWrapper'>``` to read the actual contents"]},{"cell_type":"code","metadata":{"id":"oU_37TIvD-Dl"},"source":["## let read all the lines and put into a list\n","\n","## let's see what the first line of file contains\n","\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y72rctb5D-Gu"},"source":["## Now let's place clients and decisions into variables called client and decision\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yuvCxJoD-KT"},"source":["## let's remove the word client and the extra line\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opp5s8STjCye"},"source":["## We don't want an entire sentence â€“ just what the decision was.\n","## we just want to know the status of lease in one word renew or terminate\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hEhNKkXjYHT"},"source":["## We want to store in a list to export as CSV file\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isxnLpPRlCRa"},"source":["### Confirm where we are path-wise"]},{"cell_type":"code","metadata":{"id":"D-Qc3BZ8lCRa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k9KTw-enlCRb"},"source":["### Create new results directory (note we come out of the downloaded_files folder first)"]},{"cell_type":"code","metadata":{"id":"WglOFfLHlCRb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"otpHcPn4lCRb"},"source":["### cd into our results folder\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKYRUg24lCRb"},"source":["### Confirm we are in the results folder"]},{"cell_type":"code","metadata":{"id":"F-AMX1rIlCRb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sIw6MAQNkazG"},"source":["## Export as CSV\n","\n","## use pandas to write to csv file\n","## we already imported pandas as pd\n","filename = \"lease_decisions.csv\" ## what are file name is\n","df = pd.DataFrame(decisions) ## we turn our list of dicts into a dataframe which we're call df\n","df\n","df.to_csv(filename, encoding='utf-8', index=False) ## export to csv as utf-8 coding (it just has to be this)\n","print(f\"{filename} is in your results folder!\") ## a print out that tells us the file is ready"],"execution_count":null,"outputs":[]}]}